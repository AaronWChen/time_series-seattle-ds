{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import gauss as gs\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White Noise\n",
    "\n",
    "The idea behind white noise is that it is truly random.\n",
    "\n",
    "We don't want white noise to describe our model per se, but we *do* want it to describe our model *error*.\n",
    "\n",
    "Can you explain these truisms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some white noise!\n",
    "\n",
    "rands = []\n",
    "for _ in range(1000):\n",
    "    rands.append(gs(0, 1))\n",
    "series = pd.Series(rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 1000)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(X, series);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we do a seasonal decomposition on this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pd.to_datetime(series.index, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.index = time_index\n",
    "decomp_white = sm.tsa.seasonal_decompose(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(411)\n",
    "plt.plot(series.index, decomp_white.observed, label='Original')\n",
    "plt.legend()\n",
    "plt.subplot(412)\n",
    "plt.plot(series.index, decomp_white.trend, label='Trend')\n",
    "plt.legend()\n",
    "plt.subplot(413)\n",
    "plt.plot(series.index, decomp_white.seasonal, label='Seasonal')\n",
    "plt.legend()\n",
    "plt.subplot(414)\n",
    "plt.plot(series.index, decomp_white.resid, label='Residual')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(hspace=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pd.read_csv('data/google-trends_football_us.csv').iloc[1:, :]\n",
    "fb.columns = ['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'] = fb['counts'].replace('<1', '0')\n",
    "fb['counts'] = fb['counts'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series as Both Predictor and Target?\n",
    "\n",
    "Often, the phenomenon we want to capture with a time series is a dataset being correlated with *itself*.\n",
    "\n",
    "Well, of course every dataset is perfectly correlated with itself. But what we're after now is the idea that a series is correlated with *earlier versions* of itself.\n",
    "\n",
    "Consider the problem of trying to predict tomorrow's closing price for some stock on the market. One may consider lots of features, like what sort of company it is to which the stock belongs or whether that company has been in the news recently.\n",
    "\n",
    "But it is very often the case that one of the most helpful predictors of tomorrow's price is *today's* price. And so we want to build a model where one of our predictors is an earlier version of our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.rolling(window=2).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['roll_avg'] = fb.rolling(window=2).mean()\n",
    "\n",
    "fb.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(fb[['roll_avg']][1:], fb['counts'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(fb.index[1:25], fb['counts'][1:25], label='Data')\n",
    "plt.plot(fb.index[1:25], lr.predict(fb[['roll_avg']][1:25]), label='Predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial Autocorrelation Functions\n",
    "\n",
    "Pandas and statsmodels offer autocorrelation (ACF) and partial autocorrelation (PACF) plotting tools. The idea here is to look at the correlation of a series with itself for some particular interval or *lag*. The key difference between the full and the partial autocorrelation functions is that the partial autocorrelation function ignores intervening intervals. For more, see [this post](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(fb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "plot_pacf(fb, lags=20, alpha=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'AR' is for \"Auto-Regressive\": The prediction for today will be a function of the value for previous days.\n",
    "\n",
    "The number of lag periods we want to include will be a parameter in the statsmodels model object (\"p\"). Looking at the PACF can help us decide on an appropriate p.\n",
    "\n",
    "'MA' is for \"Moving Average\": The prediction for today will be a function of the rolling mean.\n",
    "\n",
    "The number of average terms we want to include will be a parameter in the statsmodels model object (\"q\"). Looking at the ACF can help us decide on an appropriate q.\n",
    "\n",
    "For some technical details, see [this page](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model#Choosing_p_and_q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity and the Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARMA models assume that the time series is *stationary*, which means that its statistical properties are not a (meaningful) function of time.\n",
    "\n",
    "It may seem counterintuitive that, for modeling purposes, we want our time series not to be a function of time! But the basic idea is the familiar one that we want our datapoints to be mutually *independent*. For more on this topic, see [here](https://stats.stackexchange.com/questions/19715/why-does-a-time-series-have-to-be-stationary).\n",
    "\n",
    "One way of testing for stationarity is to use the Dickey-Fuller Test. The statsmodels version returns the test statistic and a p-value, relative to the null hypothesis that the series in question is NOT stationary. For more, see [this Wikipedia page](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presumably, our football series is not stationary. Let's check.\n",
    "\n",
    "adfuller(fb['counts'], autolag=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's check the stationarity of the *differences* of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].diff().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(fb['counts'].diff()[1:], autolag=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2\n",
    "q=0\n",
    "ar = ARMA(fb.diff().values[2:], (p, q)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.cumsum()`\n",
    "Let's use `np.cumsum()` to add up our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(fb['counts']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = ar.predict()\n",
    "full = fb['counts'].values[0] +  np.cumsum(preds)\n",
    "\n",
    "f, a = plt.subplots()\n",
    "a.plot(fb.index[1:15], fb[1:15], 'r', label='Data')\n",
    "a.plot(fb.index[1:15], full[1:15], 'k', label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(fb.index[1:30], fb['counts'].diff()[1:30], label='Data')\n",
    "a.plot(fb.index[1:30], preds[1:30], label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['year_q', 'unemp_rate']\n",
    "data['unemp_rate'] = data['unemp_rate'].map(lambda x:\\\n",
    "                                            float(str(x).replace('%', '')))\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['year_q']).dt.to_period('Q')\n",
    "data.set_index('date', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2\n",
    "q=0\n",
    "ar2 = ARMA(data['unemp_rate'].diff()[1:].values, (p, q)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 = ar2.predict()\n",
    "full2 = data['unemp_rate'].values[0] + np.cumsum(preds2)\n",
    "\n",
    "f, a = plt.subplots()\n",
    "a.plot(data.index.to_timestamp()[1:], data['unemp_rate'][1:], 'r', label='Data')\n",
    "a.plot(data.index.to_timestamp()[1:], full2, 'k', label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(data.index.to_timestamp()[1:], data['unemp_rate'].diff()[1:], label='Data')\n",
    "a.plot(data.index.to_timestamp()[1:], preds2, label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
